# -*- coding: utf-8 -*-
"""proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zTYnuU-1SHkqnv0xjFDKJo5egFt8cGCc
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Lambda, Input, Flatten
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
from glob import glob

import requests
import zipfile
import os
import kagglehub

url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
filename = "diabetes.csv"

# Download
r = requests.get(url)
open(filename, 'wb').write(r.content)

path = kagglehub.dataset_download("uraninjo/augmented-alzheimer-mri-dataset")

print("Path to dataset files:", path)

from sklearn.model_selection import train_test_split

# Load the dataset
diabetes_df = pd.read_csv('diabetes.csv', header=None)

# Split the data into features (X) and target (y)
X = diabetes_df.iloc[:, :-1]
y = diabetes_df.iloc[:, -1]

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Concatenate features and target for saving
train_df = pd.concat([X_train, y_train], axis=1)
test_df = pd.concat([X_test, y_test], axis=1)

# Save the training and testing data to new CSV files
train_df.to_csv('diabetes_train.csv', index=False, header=False)
test_df.to_csv('diabetes_test.csv', index=False, header=False)

print("Training data saved to diabetes_train.csv")
print("Testing data saved to diabetes_test.csv")

image_size = [224, 224]
Inception = InceptionV3(input_shape=image_size + [3], weights='imagenet', include_top=False)

for layer in Inception.layers:
  layer.trainable = False

train_path = "/content/diabetes_train.csv"
test_path = "/content/diabetes_test.csv"

folder = glob('/content/diabetes_train.csv')

x = Flatten()(Inception.output)

prediction = Dense(len(folder), activation='softmax')(x)
model = Model(inputs=Inception.input, outputs=prediction)
model.summary()

model.compile(
    loss= 'categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
    )

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)
test_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('/content/diabetes_train.csv',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')

r = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=5
)

"""## Data exploration and preprocessing

### Subtask:
Analyze the dataset for missing values, outliers, or inconsistencies. Consider feature scaling or normalization.

**Reasoning**:
Check for missing values and display descriptive statistics to analyze the dataset for missing values, outliers, and inconsistencies as per instructions 1 and 2.
"""

print("Missing values per column:")
print(diabetes_df.isnull().sum())

print("\nDescriptive statistics:")
display(diabetes_df.describe())

"""**Reasoning**:
There are no missing values based on the output of the previous step. The descriptive statistics show some columns (1, 2, 3, 4, 5) have minimum values of 0, which may not be biologically plausible for those features and could be considered outliers or missing data encoded as 0. Feature scaling is also appropriate given the varying ranges of the features. I will use `StandardScaler` to scale the features (excluding the target column) as per instructions 3 and 4.


"""

from sklearn.preprocessing import StandardScaler

# Identify columns with potential outliers (0 values that are not plausible)
# Based on domain knowledge for diabetes, 0 glucose, blood pressure, skin thickness, insulin, and BMI are not realistic.
cols_with_outliers_as_zero = [1, 2, 3, 4, 5]

# Replace 0 values in these columns with NaN
for col in cols_with_outliers_as_zero:
    diabetes_df[col] = diabetes_df[col].replace(0, np.nan)

# Impute missing values with the mean of each column
diabetes_df.fillna(diabetes_df.mean(), inplace=True)

# Separate features (X) and target (y) again after imputation
X = diabetes_df.iloc[:, :-1]
y = diabetes_df.iloc[:, -1]

# Apply StandardScaler to the feature columns
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert the scaled features back to a DataFrame for potential future use
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Display the first few rows of the scaled features and descriptive statistics to verify
display(X_scaled_df.head())
display(X_scaled_df.describe())

"""## Model architecture

### Subtask:
Experiment with different model architectures, such as adding more layers, changing the number of neurons per layer, or trying different activation functions.

**Reasoning**:
Define a new Sequential model with multiple Dense layers and compile it for binary classification.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define a new Sequential model
model = Sequential()

# Add Dense layers
model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))

# Add the output layer
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
    )

model.summary()

"""## Hyperparameter tuning

### Subtask:
Tune hyperparameters like the learning rate, batch size, and number of epochs.

**Reasoning**:
Compile the model with a different learning rate for the Adam optimizer and then train the model with a different batch size and number of epochs. After training, evaluate the model's performance on the test data.
"""

from tensorflow.keras.optimizers import Adam

# Compile the model with a different learning rate
model.compile(
    loss='binary_crossentropy',
    optimizer=Adam(learning_rate=0.001),
    metrics=['accuracy']
)

# Train the model with a different batch size and number of epochs
r = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=64
)

# Evaluate the model's performance on the test data
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

"""## Regularization

### Subtask:
Implement techniques like L1 or L2 regularization or dropout to prevent overfitting.

**Reasoning**:
Implement dropout layers in the model to prevent overfitting and then compile and train the model.
"""

from tensorflow.keras.layers import Dropout

# Define a new Sequential model with dropout layers
model = Sequential()

# Add Dense layers with dropout
model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.5)) # Add dropout with a rate of 0.5
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3)) # Add dropout with a rate of 0.3
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2)) # Add dropout with a rate of 0.2

# Add the output layer
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

model.summary()

# Train the regularized model
r_regularized = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=64
)

# Evaluate the performance of the regularized model
loss_regularized, accuracy_regularized = model.evaluate(X_test, y_test, verbose=0)
print(f"Regularized Model Test Loss: {loss_regularized:.4f}")
print(f"Regularized Model Test Accuracy: {accuracy_regularized:.4f}")

"""## Cross-validation

### Subtask:
Use cross-validation to get a more reliable estimate of the model's performance and to ensure it generalizes well to unseen data.

**Reasoning**:
Initialize StratifiedKFold and lists to store cross-validation scores.
"""

from sklearn.model_selection import StratifiedKFold

n_splits = 5  # Number of folds
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
cvscores = []

"""**Reasoning**:
Loop through each fold, split the data, define, compile, fit, and evaluate the model for each fold, then store the scores.


"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Assuming X_scaled and y are already available from previous steps
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Define the model architecture within the loop
    model = Sequential()
    model.add(Dense(128, activation='relu', input_shape=(X_train_fold.shape[1],)))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Fit the model
    model.fit(X_train_fold, y_train_fold, epochs=100, batch_size=64, verbose=0)

    # Evaluate the model
    scores = model.evaluate(X_test_fold, y_test_fold, verbose=0)
    print(f"Accuracy for fold: {scores[1]*100:.2f}%")
    cvscores.append(scores[1] * 100)

"""**Reasoning**:
Calculate and print the mean and standard deviation of the cross-validation accuracies.


"""

print(f"Mean Accuracy: {np.mean(cvscores):.2f}%")
print(f"Standard Deviation of Accuracy: {np.std(cvscores):.2f}%")

"""## Ensemble methods

### Subtask:
Explore using ensemble methods like Random Forests or Gradient Boosting.

**Reasoning**:
Import the necessary libraries for Random Forest and evaluation metrics, then instantiate, train, predict, and evaluate the Random Forest model using the previously scaled training and test data.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Instantiate a RandomForestClassifier model
# Using default parameters initially
rf_model = RandomForestClassifier(random_state=42)

# Train the Random Forest model on the scaled training data
# Assuming X_scaled is available from previous steps and matches the original data order before train_test_split
# We will use X_train and y_train from the train_test_split performed earlier
rf_model.fit(X_train, y_train)

# Predict on the scaled test data
y_pred_rf = rf_model.predict(X_test)

# Evaluate the performance of the Random Forest model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

# Print the evaluation metrics
print("Random Forest Model Evaluation:")
print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-score: {f1_rf:.4f}")

"""## Algorithm selection

### Subtask:
Try different classification algorithms to see if another model performs better on your dataset.

**Reasoning**:
Import the required classification algorithms and accuracy metric.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

"""**Reasoning**:
Create instances of Logistic Regression, SVC, and K-Nearest Neighbors classifiers, train them on the training data, make predictions on the test data, and calculate the accuracy for each model.


"""

# Create instances of the classifiers
lr_model = LogisticRegression(random_state=42)
svc_model = SVC(kernel='linear', random_state=42)
knn_model = KNeighborsClassifier(n_neighbors=5)

# Train the models
lr_model.fit(X_train, y_train)
svc_model.fit(X_train, y_train)
knn_model.fit(X_train, y_train)

# Make predictions
y_pred_lr = lr_model.predict(X_test)
y_pred_svc = svc_model.predict(X_test)
y_pred_knn = knn_model.predict(X_test)

# Evaluate accuracy
accuracy_lr = accuracy_score(y_test, y_pred_lr)
accuracy_svc = accuracy_score(y_test, y_pred_svc)
accuracy_knn = accuracy_score(y_test, y_pred_knn)

# Print the accuracy scores
print(f"Logistic Regression Accuracy: {accuracy_lr:.4f}")
print(f"SVC (Linear Kernel) Accuracy: {accuracy_svc:.4f}")
print(f"K-Nearest Neighbors Accuracy: {accuracy_knn:.4f}")

"""# Task
Create a Python script that takes user input for 8 features related to diabetes prediction, preprocesses the input, and uses a pre-trained model ("diabetes_model.pkl") and scaler ("scaler.pkl") to predict whether the person is diabetic or not. The script should then display the prediction to the user.

## Get user input

### Subtask:
Prompt the user to enter values for each of the 8 features.

**Reasoning**:
Define the feature names, initialize an empty list, and loop through the feature names to get user input for each feature, converting the input to float and storing it in the list.
"""

# 1. Define a list of the 8 feature names and their hint ranges
feature_names_with_hints = [
    ('Pregnancies', '0-17'),
    ('Glucose', '44-199'),
    ('BloodPressure', '24-122'),
    ('SkinThickness', '7-99'),
    ('Insulin', '14-846'),
    ('BMI', '18.2-67.1'),
    ('DiabetesPedigreeFunction', '0.078-2.42'),
    ('Age', '21-81')
    ]

# 2. Initialize an empty list to store the user's input values
user_input_values = []

# 3. Loop through the list of feature names and prompt for input with hints
for feature_name, hint_range in feature_names_with_hints:
    while True:
        try:
            value_str = input(f"Enter value for {feature_name} ({hint_range}): ")
            # 4. Convert the user's input to a numerical type (float) and append
            value = float(value_str)
            user_input_values.append(value)
            break # Exit the while loop if conversion is successful
        except ValueError:
            print("Invalid input. Please enter a numerical value.")

print("\nUser input collected:", user_input_values)

"""## Preprocess input

### Subtask:
Convert the user input into a format suitable for the model, including handling potential zero values and scaling using the previously fitted scaler.

**Reasoning**:
Convert the user input to a NumPy array, handle potential zero values by replacing them with NaN, impute NaN values with the mean from the training data (which needs to be calculated or accessed), and then scale the imputed values using the fitted scaler.
"""

# 1. Convert user input list to a NumPy array and reshape
user_input_array = np.array(user_input_values).reshape(1, -1)

# 2. Identify columns with potential outliers as zero
# These indices correspond to the original diabetes_df columns (0-indexed)
cols_with_outliers_as_zero_indices = [1, 2, 3, 4, 5] # Glucose, BloodPressure, SkinThickness, Insulin, BMI

# 3. Replace 0 values in the user input array for these columns with NaN
for col_index in cols_with_outliers_as_zero_indices:
    if user_input_array[0, col_index] == 0:
        user_input_array[0, col_index] = np.nan

# 4. Impute missing values (NaNs) with the mean of each column from the training data
# Need to access the means of the original training data (X_train) for imputation
# Calculate means from X_train (assuming X_train is available from previous steps)
training_means = X_train.mean()

for col_index in cols_with_outliers_as_zero_indices:
    if np.isnan(user_input_array[0, col_index]):
        user_input_array[0, col_index] = training_means[col_index]

# 5. Use the previously fitted scaler to transform the preprocessed user input array
# Assuming 'scaler' object is available from the preprocessing step
user_input_scaled = scaler.transform(user_input_array)

print("\nPreprocessed and scaled user input:", user_input_scaled)

"""**Reasoning**:
The preprocessed and scaled user input is ready for prediction. Use the loaded model to predict on the scaled input and display the prediction result.


"""

# Assuming 'model' is the trained model object (either the Keras model or the best performing sklearn model)

# For the best performing sklearn model (SVC)
# Assuming svc_model is the trained SVC model from the previous step
prediction = svc_model.predict(user_input_scaled)

# Define a function to display the prediction and suggestions
def suggestions(prediction, user_input_values, feature_names):
    """Displays the diabetes prediction and provides suggestions based on input values.

    Args:
        prediction: The prediction from the model (0 or 1).
        user_input_values: A list of the original user input values.
        feature_names: A list of the feature names.
    """
    if prediction[0] == 1:
        print("\nPrediction: Based on the provided information, the person is likely diabetic.")
        print("It is highly recommended to consult a doctor for a proper diagnosis and personalized treatment plan.")
        print("\nHere are some general suggestions based on your input values:")

        # Access the original user input values for targeted suggestions
        # Using indices as before, assuming the order is consistent

        # Suggestion for high Glucose (index 1)
        if user_input_values[1] > 120: # Using a threshold as an example
            print(f"- Your {feature_names[1][0]} level is high. Consider reducing sugar intake and refined carbohydrates.")

        # Suggestion for high BloodPressure (index 2)
        if user_input_values[2] > 80: # Using a threshold as an example
            print(f"- Your {feature_names[2][0]} is high. Focus on a low-sodium diet and regular physical activity.")

        # Suggestion for high BMI (index 5)
        if user_input_values[5] > 25: # Using a threshold as an example
            print(f"- Your {feature_names[5][0]} is in the higher range. Aim for weight management through a balanced diet and exercise.")

        # Suggestion for low BMI (though less common for diabetes, can still provide general health advice)
        if user_input_values[5] < 18.5: # Using a threshold as an example
             print(f"- Your {feature_names[5][0]} is in the lower range. Ensure you are consuming nutritious food to maintain a healthy weight.")


        # General suggestion for exercise
        print("- Incorporate regular physical activity into your routine.")
        print("- Maintain a healthy and balanced diet.")

    else:
        print("\nPrediction: Based on the provided information, the person is likely not diabetic.")
        print("Suggestion: Maintaining a healthy lifestyle with balanced diet and regular exercise is always beneficial for preventing diabetes.")

# Call the function to display the prediction and suggestions
# Assuming feature_names_with_hints is available from the input collection step
suggestions(prediction, user_input_values, feature_names_with_hints)



"""# Task
Create a GUI application using tkinter that takes 8 input values related to diabetes prediction, provides hints for the expected range of values, predicts whether the person is diabetic or not using a pre-trained model, and based on the input and prediction, suggests lifestyle changes or the need to consult a doctor, and provides tasks to help reduce diabetes problems.

## Import necessary libraries

### Subtask:
Import `tkinter` for creating the GUI, along with `numpy` for data handling, and the necessary components from `sklearn` and `tensorflow` for loading and using the pre-trained model and scaler.

**Reasoning**:
Import the necessary libraries for building the GUI application, including tkinter, numpy, and components for loading and using the pre-trained model and scaler.
"""

import tkinter as tk
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from joblib import load

"""## Create the main application window

### Subtask:
Initialize the Tkinter window.

"""

import tkinter as tk
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from joblib import load
import os

# Define the feature names and their hint ranges
feature_names_with_hints = [
    ('Pregnancies', '0-17'),
    ('Glucose', '44-199'),
    ('BloodPressure', '24-122'),
    ('SkinThickness', '7-99'),
    ('Insulin', '14-846'),
    ('BMI', '18.2-67.1'),
    ('DiabetesPedigreeFunction', '0.078-2.42'),
    ('Age', '21-81')
]

# Assume the model and scaler are saved as 'diabetes_model.pkl' and 'scaler.pkl'
# Make sure these files are in the same directory as your script when you run it locally
try:
    model = load('diabetes_model.pkl')
    scaler = load('scaler.pkl')
except FileNotFoundError:
    print("Error: Make sure 'diabetes_model.pkl' and 'scaler.pkl' are in the same directory.")
    # You might want to exit or handle this error more gracefully in a real application
except Exception as e:
    print(f"An error occurred loading the model or scaler: {e}")
    # Handle other potential loading errors

# Assume you have calculated the training means and saved them or have access to them
# For this example, let's use the means from the original diabetes_df (before train_test_split)
# In a real scenario, you would save these means during your training process
# Here, we'll calculate them again for demonstration purposes.
# Make sure diabetes_df is loaded or accessible if you run this cell independently.
try:
    # This assumes diabetes_df is available. If not, you'd need to load it or save the means separately.
    # Replace 0s with NaN for calculating means of relevant columns
    temp_df = pd.read_csv('diabetes.csv', header=None)
    cols_for_means = [1, 2, 3, 4, 5]
    for col in cols_for_means:
         temp_df[col] = temp_df[col].replace(0, np.nan)
    training_means = temp_df.mean()
except FileNotFoundError:
    print("Error: diabetes.csv not found. Cannot calculate training means for imputation.")
    training_means = None # Handle this case in the predict function
except Exception as e:
    print(f"An error occurred calculating training means: {e}")
    training_means = None


def predict_diabetes():
    """Retrieves input, preprocesses, predicts, and displays results."""
    if model is None or scaler is None or training_means is None:
        result_label.config(text="Error: Model, scaler, or training means not loaded.")
        return

    user_input_values = []
    try:
        for i in range(8):
            value = float(entries[i].get())
            user_input_values.append(value)
    except ValueError:
        result_label.config(text="Invalid input. Please enter numerical values.")
        return

    # Preprocess input (same logic as before)
    user_input_array = np.array(user_input_values).reshape(1, -1)

    cols_with_outliers_as_zero_indices = [1, 2, 3, 4, 5]
    for col_index in cols_with_outliers_as_zero_indices:
        if user_input_array[0, col_index] == 0:
            user_input_array[0, col_index] = np.nan

    # Impute missing values
    for col_index in cols_with_outliers_as_zero_indices:
        if np.isnan(user_input_array[0, col_index]):
            user_input_array[0, col_index] = training_means[col_index]


    user_input_scaled = scaler.transform(user_input_array)

    # Make prediction
    prediction = model.predict(user_input_scaled)

    # Display result and suggestions
    suggestions_text = ""
    if prediction[0] == 1:
        suggestions_text += "Prediction: Based on the provided information, the person is likely diabetic.\n"
        suggestions_text += "It is highly recommended to consult a doctor for a proper diagnosis and personalized treatment plan.\n"
        suggestions_text += "\nHere are some general suggestions based on your input values:\n"

        if user_input_values[1] > 120:
            suggestions_text += f"- Your {feature_names_with_hints[1][0]} level is high. Consider reducing sugar intake and refined carbohydrates.\n"
        if user_input_values[2] > 80:
            suggestions_text += f"- Your {feature_names_with_hints[2][0]} is high. Focus on a low-sodium diet and regular physical activity.\n"
        if user_input_values[5] > 25:
            suggestions_text += f"- Your {feature_names_with_hints[5][0]} is in the higher range. Aim for weight management through a balanced diet and exercise.\n"
        if user_input_values[5] < 18.5:
             suggestions_text += f"- Your {feature_names_with_hints[5][0]} is in the lower range. Ensure you are consuming nutritious food to maintain a healthy weight.\n"

        suggestions_text += "- Incorporate regular physical activity into your routine.\n"
        suggestions_text += "- Maintain a healthy and balanced diet.\n"

    else:
        suggestions_text += "Prediction: Based on the provided information, the person is likely not diabetic.\n"
        suggestions_text += "Suggestion: Maintaining a healthy lifestyle with balanced diet and regular exercise is always beneficial for preventing diabetes.\n"

    result_label.config(text=suggestions_text)


# Create the main Tkinter window
root = tk.Tk()
root.title("Diabetes Prediction")

# Create labels and entry fields for each feature
entries = []
for i, (name, hint) in enumerate(feature_names_with_hints):
    label = tk.Label(root, text=f"{name} ({hint}):")
    label.grid(row=i, column=0, padx=5, pady=5, sticky="w")
    entry = tk.Entry(root)
    entry.grid(row=i, column=1, padx=5, pady=5, sticky="ew")
    entries.append(entry)

# Create a button to trigger the prediction
predict_button = tk.Button(root, text="Predict Diabetes", command=predict_diabetes)
predict_button.grid(row=8, column=0, columnspan=2, pady=10)

# Create a label to display the prediction result and suggestions
result_label = tk.Label(root, text="", justify="left")
result_label.grid(row=9, column=0, columnspan=2, padx=5, pady=5, sticky="w")

# Configure column weights so the entry fields expand
root.grid_columnconfigure(1, weight=1)

# Run the Tkinter event loop
root.mainloop()